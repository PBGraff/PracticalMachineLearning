---
title: "Practical Machine Learning Course Project:  Weight Lifting Exercise"
author: "Philip Graff"
date: "September 15, 2014"
output: html_document
---

## Introduction
There are currently many self-monitoring devices on the market, such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit*. These allow the wearers to collect a large amount of data about their personal activity and have helped start the 'quantified self' movement. In this project, we analyze the data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants as they perform barbell lifts in 5 different ways. Using this recorded data, we create a model and predict how lift was performed.

The five methods are as follows:

1. exactly according to the specification (A)
1. throwing the elbows to the front (B)
1. lifting the dumbbell only halfway (C)
1. lowering the dumbbell only halfway (D)
1. throwing the hips to the front (E)

More information about the data is available from <http://groupware.les.inf.puc-rio.br/har> and the paper:  Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

## Loading packages
Several packages will be needed for this analysis. Let's load them in the beginning to have the tools available throughout.
```{r,results='hide',cache=TRUE,warning=FALSE}
require(ggplot2,quietly=TRUE)
require(lattice,quietly=TRUE)
require(caret,quietly=TRUE)
require(rattle,quietly=TRUE)
require(rpart,quietly=TRUE)
require(randomForest,quietly=TRUE)
require(plyr,quietly=TRUE)
require(gbm,quietly=TRUE)
require(e1071,quietly=TRUE)
require(klaR,quietly=TRUE)
require(kernlab,quietly=TRUE)
require(deepnet,quietly=TRUE)
```

## Obtaining and Cleaning the Data
We begin by downloading the training and test data (tested 15 Sept 2014) and then loading it into data frames.
```{r,results='hide',cache=TRUE}
if( !file.exists("pml-training.csv") )
        download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile="pml-training.csv",method="curl")
if( !file.exists("pml-testing.csv") )
        download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile="pml-testing.csv",method="curl")
training <- read.csv("pml-training.csv",header=TRUE)
testing <-  read.csv("pml-testing.csv",header=TRUE)
```

Many columns are mostly 'NA' values while others have none, so we begin by eliminating those with 'NA' values. We then eliminate columns that are timestamps and window information, since these are irrelevant for our task. We also eliminate the first two columns, as they are just a trial number and name of the lifter.
```{r,results='hide',cache=TRUE}
colKeep <- colSums(is.na(training))==0
training <- training[,colKeep]
testing <- testing[,colKeep]
timeCols <- grep("timestamp|window",names(training))
training <- training[,-c(1,2,timeCols)]
testing <- testing[,-c(1,2,timeCols)]
```

In our final removal of possible predictors, we eliminate those variables that have a near-zero variance. As these do not vary significantly between different trials, they will be poor predictors.
```{r,results='hide',cache=TRUE}
nzv <- nearZeroVar(training)
training <- training[,-nzv]
testing <- testing[,-nzv]
```
This leaves us with 52 predictor variables to use to predict the 'classe' variable, which is the method in whch the lift was performed.

## Exploratory Analysis
For an initial analysis of the complexity of the problem, we begin by creating a subset of a random 5% of the data. With this data, we fit a single decision tree and analyze this to determine how well it performs.
```{r,cache=TRUE}
set.seed(1867)
smallTrain<-training[createDataPartition(training$classe,p=0.05,list=FALSE),]
modExpFit<-train(smallTrain$classe~.,data=smallTrain,method="rpart")
```
```{r,cache=TRUE,echo=FALSE}
fancyRpartPlot(modExpFit$finalModel,sub = "Sample Decision Tree")
confusionMatrix(smallTrain$classe,predict(modExpFit,smallTrain))
```
It is clear from the confusion matrix that this classification method does only moderately well. A more sophisticated approach is required.

## Fitting Models
In this seciton, we will fit and test various models in order to find which performs the best.

### Evaluation data
The first thing we need to do, however, is create a subset of the training data that can be used for evaluating the model fits and comparing against one another. Accuracy on this set will determine which is used for the final 20 examples.
```{r,cache=TRUE}
set.seed(1066)
inTrain <- createDataPartition(training$classe,p=0.8,list=FALSE)
trainDat <- training[inTrain,]
testDat <- training[-inTrain,]
```

### Random Forests
The first model we train on the data is a random forest. We use the default setting of 500 trees and use 10-fold cross-validation for model fitting, where in each fold 90% of data is used for training and the remaining 10% for evaluation. The stats on this model fit are displayed below. We see that the random forest performs very well, with an out-of-bag error of only 0.57%.
```{r,cache=TRUE}
set.seed(12345)
modRF <- train(classe~., data=trainDat, method="rf", trControl = trainControl(method = "cv", number = 10))
```
```{r,cache=TRUE,echo=FALSE}
modRF$finalModel
```

This random forest must be evaluated on the test data we prepared for proper comparison with other methods.
```{r,cache=TRUE}
confMatRF <- confusionMatrix(testDat$classe,predict(modRF,testDat))
```
```{r,cache=TRUE,echo=FALSE}
confMatRF
```
We see an accuracy of **`r confMatRF$overall[1]*100`%** for the random forest method.

### Generalized Boosted Modeling with Trees
The next model to fit to the data involves boosted decision trees via the GBM method. This will train many decision trees sequentially, each time increasing the weight of data points predicted incorrectly the previous time and decreasing those that were predicted correctly. By combining the many trees, this produces a stronger predictor. Furthermore, GBM uses greedy selection of basis functions to improve classification. Cross-validation is used with 10 folds.
```{r,cache=TRUE}
set.seed(54321)
modGBM <- train(classe~., data=trainDat, method="gbm", verbose = FALSE, trControl = trainControl(method = "cv", number = 10))
```
```{r,cache=TRUE,echo=FALSE}
modGBM
```

This ensemble of boosted trees must then be evaluated on the test data we prepared for proper comparison with other methods.
```{r,cache=TRUE}
confMatGBM <- confusionMatrix(testDat$classe,predict(modGBM,testDat))
```
```{r,cache=TRUE,echo=FALSE}
confMatGBM
```
We see an accuracy of **`r confMatGBM$overall[1]*100`%** for generalized boosted modeling with trees.

### Naive Bayes
The next machine learning method that we try is Naive Bayes. In this, each variable is assumed to be independent to fit a probability density model to the training data. We first use Naive Bayes on the raw parameter values and then train a similar model using principal component analysis for pre-processing. The PCA may help by creating more useful and more independent features to use for training.
```{r,cache=TRUE}
set.seed(9069)
modNB <- train(classe~., data=trainDat, method="nb", trControl = trainControl(method = "cv", number = 10))
set.seed(1208)
modNBpca <- train(classe~., data=trainDat, method="nb", preProcess = "pca", trControl = trainControl(method = "cv", number = 10))
```

These two Naive Bayes models are now evaluated on the test data set in order for proper comparison to the other results.
```{r,cache=TRUE}
confMatNB <- confusionMatrix(testDat$classe,predict(modNB,testDat))
confMatNBpca <- confusionMatrix(testDat$classe,predict(modNBpca,testDat))
```
```{r,cache=TRUE,echo=FALSE}
confMatNB
confMatNBpca
```
We see an accuracy of **`r confMatNB$overall[1]*100`%** for plain Naive Bayes and **`r confMatNBpca$overall[1]*100`%** for Naive Bayes with PCA pre-processing.

### Linear and Quadratic Discriminant Analysis
Our final set of methods of classification is linear and quadratic discriminant analysis. This will form linear or quadratic decision boundaries in the input feature space. These are determined by fitting multivariate Guassians to each class. Linear discriminant analysis requires that they all use the same covariance matrix, while quadratic discriminant analysis allows the covariances to vary. We then repeat these methods using PCA pre-processing.
```{r,cache=TRUE}
set.seed(31269)
modLDA <- train(classe~., data=trainDat, method="lda", trControl = trainControl(method = "cv", number = 10))
set.seed(1448)
modLDApca <- train(classe~., data=trainDat, method="lda", preProcess = "pca", trControl = trainControl(method = "cv", number = 10))
set.seed(57348)
modQDA <- train(classe~., data=trainDat, method="qda", trControl = trainControl(method = "cv", number = 10))
set.seed(342)
modQDApca <- train(classe~., data=trainDat, method="qda", preProcess = "pca", trControl = trainControl(method = "cv", number = 10))
```

These four models are now evaluated on the test data set in order for proper comparison to the other results.
```{r,cache=TRUE}
confMatLDA <- confusionMatrix(testDat$classe,predict(modLDA,testDat))
confMatLDApca <- confusionMatrix(testDat$classe,predict(modLDApca,testDat))
confMatQDA <- confusionMatrix(testDat$classe,predict(modQDA,testDat))
confMatQDApca <- confusionMatrix(testDat$classe,predict(modQDApca,testDat))
```
```{r,cache=TRUE,echo=FALSE}
confMatLDA
confMatLDApca
confMatQDA
confMatQDApca
```
We see an accuracy of **`r confMatLDA$overall[1]*100`%** for linear discriminant analysis and **`r confMatLDApca$overall[1]*100`%** for LDA with PCA pre-processing; we see an accuracy of **`r confMatQDA$overall[1]*100`%** for quadratic discriminant analysis and **`r confMatQDApca$overall[1]*100`%** for QDA with PCA pre-processing.

## ROC analysis of Best Classifier
Before making predictions on the 20 test cases, we look at the receiver operating characteristic (ROC) curve from our best model. This shows the true positive rate vs the false positive rate as a function of the classification threshold.

## Predictions on 20 Test Cases
don't forget to print to file for submission!

## Conclusions
PCA does worse as it keeps only 95% of variance by default and extra 5% will be important it seems.
