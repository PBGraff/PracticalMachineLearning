---
title: "Practical Machine Learning Course Project:  Weight Lifting Exercise"
author: "Philip Graff"
date: "September 15, 2014"
output:
  html_document:
    keep_md: true
    toc: yes
---

## Introduction
There are currently many self-monitoring devices on the market, such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit*. These allow the wearers to collect a large amount of data about their personal activity and have helped start the 'quantified self' movement. In this project, we analyze the data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants as they perform barbell lifts in 5 different ways. Using this recorded data, we create a model and predict how lift was performed.

The five methods are as follows:

1. exactly according to the specification (A)
1. throwing the elbows to the front (B)
1. lifting the dumbbell only halfway (C)
1. lowering the dumbbell only halfway (D)
1. throwing the hips to the front (E)

More information about the data is available from <http://groupware.les.inf.puc-rio.br/har> and the paper:  Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

## Loading packages
Several packages will be needed for this analysis. We load them in the beginning in order to have the tools available throughout.
```{r,results='hide',cache=TRUE,warning=FALSE,load_libs}
require(ggplot2,quietly=TRUE)
require(lattice,quietly=TRUE)
require(caret,quietly=TRUE)
require(rattle,quietly=TRUE)
require(rpart,quietly=TRUE)
require(randomForest,quietly=TRUE)
require(plyr,quietly=TRUE)
require(gbm,quietly=TRUE)
require(e1071,quietly=TRUE)
require(klaR,quietly=TRUE)
```

## Obtaining and Cleaning the Data
We begin by downloading the training and test data (tested 15 Sept 2014) and then loading it into data frames.
```{r,results='hide',cache=TRUE,load_data}
if( !file.exists("pml-training.csv") )
        download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile="pml-training.csv",method="curl")
if( !file.exists("pml-testing.csv") )
        download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile="pml-testing.csv",method="curl")
training <- read.csv("pml-training.csv",header=TRUE)
testing <-  read.csv("pml-testing.csv",header=TRUE)
```

Many columns are mostly 'NA' values while others have none, so we begin by eliminating those with 'NA' values. We then eliminate columns that are timestamps and window information, since these are irrelevant for our task. We also eliminate the first two columns, as they are just a trial number and name of the lifter.
```{r,results='hide',cache=TRUE,trim_data_1}
colKeep <- colSums(is.na(training))==0
training <- training[,colKeep]
testing <- testing[,colKeep]
timeCols <- grep("timestamp|window",names(training))
training <- training[,-c(1,2,timeCols)]
testing <- testing[,-c(1,2,timeCols)]
```

In our final removal of possible predictors, we eliminate those variables that have a near-zero variance. As these do not vary significantly between different trials, they will be poor predictors.
```{r,results='hide',cache=TRUE,trim_data_2}
nzv <- nearZeroVar(training)
training <- training[,-nzv]
testing <- testing[,-nzv]
```
This leaves us with 52 predictor variables to use to predict the 'classe' variable, which is the method in whch the lift was performed.

## Exploratory Analysis
Our first look at the data is to simply histogram the occurrences of each lift method in the training data. This will show us if any are over- or under-represented.
```{r,cache=TRUE,echo=FALSE,classe_hist}
qplot(training$classe,xlab = "Lift Method",ylab = "Frequency", main = "Distribution of Lift Methods in Training Data")
```
Although method A is slightly over-represented compared to the others, it is not so drastic that it will cause problems in the training.

We then make a scatter plot of a few of the variables, in particular those from the accelerometers in the arms of the participants. The different methods are distinguished by color and it is clear from these that no simple relationship between the observed variables exists and more complex models will be needed.
```{r,cache=TRUE,echo=FALSE,pairs_plot}
featurePlot(x=training[,grep("accel_arm",names(training))],y=training$classe,plot="pairs",main="Scatter Plot of Select Training Data Variables")
```

For an initial analysis of the complexity of the problem, we begin by creating a subset of a random 5% of the data. With this data, we fit a single decision tree and analyze this to determine how well it performs.
```{r,cache=TRUE,exploratory_tree}
set.seed(1867)
smallTrain<-training[createDataPartition(training$classe,p=0.05,list=FALSE),]
modExpFit<-train(smallTrain$classe~.,data=smallTrain,method="rpart")
```
```{r,cache=TRUE,echo=FALSE,exploratory_plot_1}
fancyRpartPlot(modExpFit$finalModel,sub = "Sample Decision Tree")
confusionMatrix(smallTrain$classe,predict(modExpFit,smallTrain))
```
It is clear from the confusion matrix that this classification method does only moderately well. A more sophisticated approach is required.

## Fitting Models
In this seciton, we will fit and test various models in order to find which performs the best. For each, we set the random seed prior to fitting so that the results will be identically reproducible.

### Evaluation data
The first thing we need to do, however, is create a subset of the training data that can be used for evaluating the model fits and comparing against one another. Accuracy on this set will determine which is used for the final 20 examples.
```{r,cache=TRUE,data_separate}
set.seed(1066)
inTrain <- createDataPartition(training$classe,p=0.8,list=FALSE)
trainDat <- training[inTrain,]
trainEval <- training[-inTrain,]
```
In this report we use the accuracy as our figure of merit. This is simply 1-Err, where Err is the out-of-bag error on predictions. Confidence intervals for accuracy/error measures are obtained by performing 10-fold cross-validation in all cases.

### Random Forests
The first model we train on the data is a random forest. We use 10-fold cross-validation for model fitting, where in each fold 90% of data is used for training and the remaining 10% for evaluation. The default settings for the random forest use 500 trees and a few settings of mtry, which determines the number of parameters considered at each node in the tree. The statistics on this model fit are displayed below and we see that the random forest performs very well.
```{r,cache=TRUE,rf_fit}
set.seed(12345)
modRF <- train(classe~., data=trainDat, method = "rf", trControl = trainControl(method = "cv", number = 10))
modRF
```

The best random forest must then make predictions on the evaluation data we prepared for proper comparison with other methods.
```{r,cache=TRUE,rf_confmat}
confMatRF <- confusionMatrix(trainEval$classe,predict(modRF,trainEval))
confMatRF
```
We see an accuracy of **`r confMatRF$overall[1]*100`%** for the random forest method with a 95% confidence interval of (`r confMatRF$overall[3]*100`%,`r confMatRF$overall[4]*100`%).

### Generalized Boosted Modeling with Trees
The next model to fit to the data involves boosted decision trees via the GBM method. This will train many decision trees sequentially, each time increasing the weight of data points predicted incorrectly the previous time and decreasing those that were predicted correctly. By combining the many trees, this produces a stronger predictor. Furthermore, GBM uses greedy selection of basis functions to improve classification. Cross-validation is used with 10 folds.
```{r,cache=TRUE,gbm_fit}
set.seed(54321)
modGBM <- train(classe~., data=trainDat, method="gbm", verbose = FALSE, tuneGrid = expand.grid(n.trees=seq(100,500,by=100),interaction.depth=seq(1,5),shrinkage=0.1), trControl = trainControl(method = "cv", number = 10))
modGBM
```

This ensemble of boosted trees must then be used to make predictions on the evaluation data we prepared for proper comparison with other methods.
```{r,cache=TRUE,gbm_confmat}
confMatGBM <- confusionMatrix(trainEval$classe,predict(modGBM,trainEval))
confMatGBM
```
We see an accuracy of **`r confMatGBM$overall[1]*100`%** for generalized boosted modeling with trees with a 95% confidence interval of (`r confMatGBM$overall[3]*100`%,`r confMatGBM$overall[4]*100`%).

### Naive Bayes
The next machine learning method that we try is Naive Bayes. In this, each variable is assumed to be independent to fit a probability density model to the training data. In addition to training using the raw parameter values, a fit using principal component analysis pre-processing was done. This did not perform as well, potentially due to PCA only conserving 95% of the variance of the original data and only minial reduction in input dimension. The PCA pre-processed fit is thus omitted here.
```{r,cache=TRUE,nb_fit}
nbGrid <- expand.grid(fL=c(0,3),usekernel=c(FALSE,TRUE))
set.seed(9069)
modNB <- train(classe~., data=trainDat, method="nb", tuneGrid = nbGrid, trControl = trainControl(method = "cv", number = 10))
modNB
```

These two Naive Bayes models are now evaluated on the test data set in order for proper comparison to the other results.
```{r,cache=TRUE,nb_confmat}
confMatNB <- confusionMatrix(trainEval$classe,predict(modNB,trainEval))
confMatNB
```
We see an accuracy of **`r confMatNB$overall[1]*100`%** for Naive Bayes, with a 95% confidence interval of (`r confMatNB$overall[3]*100`%,`r confMatNB$overall[4]*100`%).

### Linear and Quadratic Discriminant Analysis
Our final set of methods of classification is linear and quadratic discriminant analysis. This will form linear or quadratic decision boundaries in the input feature space. These are determined by fitting multivariate Guassians to each class. Linear discriminant analysis requires that they all use the same covariance matrix, while quadratic discriminant analysis allows the covariances to vary. These fits were repeated with PCA pre-processing, but as with Naive Bayes there was actually a reduction in the performance so they are omitted here.
```{r,cache=TRUE,lda_fit}
set.seed(31269)
modLDA <- train(classe~., data=trainDat, method="lda", trControl = trainControl(method = "cv", number = 10))
set.seed(57348)
modQDA <- train(classe~., data=trainDat, method="qda", trControl = trainControl(method = "cv", number = 10))
modLDA
modQDA
```

These four models are now evaluated on the test data set in order for proper comparison to the other results.
```{r,cache=TRUE,lda_confmat}
confMatLDA <- confusionMatrix(trainEval$classe,predict(modLDA,trainEval))
confMatQDA <- confusionMatrix(trainEval$classe,predict(modQDA,trainEval))
confMatLDA
confMatQDA
```
We see an accuracy of **`r confMatLDA$overall[1]*100`%** for linear discriminant analysis and **`r confMatQDA$overall[1]*100`%** for quadratic discriminant analysis. LDA has a 95% confidence interval of (`r confMatLDA$overall[3]*100`%,`r confMatLDA$overall[4]*100`%) and for QDA this is (`r confMatQDA$overall[3]*100`%,`r confMatQDA$overall[4]*100`%).

## ROC analysis of Best Classifier
Before making predictions on the 20 test cases, we look at the receiver operating characteristic (ROC) curve from our best model. This shows the true positive rate vs the false positive rate as a function of the classification threshold.

## Predictions on 20 Test Cases
From the comparisons of predictions on the evaluation data set, we see that the random forests method produced the best results, with greater than 99% accuracy. We then use this to make predictions for the testing data set with its 20 observations.
```{r,cache=TRUE,test_eval}
testAnswers <- as.character(predict(modRF$best.model,testing[,-53]))
testAnswers
```
These are then printed to individual text files for submission.
```{r,cache=TRUE,echo=FALSE,test_eval_print}
pml_write_files <- function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(testAnswers)
```

## Conclusions
PCA does worse as it keeps only 95% of variance by default and extra 5% will be important it seems.
